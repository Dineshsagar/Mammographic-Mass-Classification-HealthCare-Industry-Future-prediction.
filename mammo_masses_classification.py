# -*- coding: utf-8 -*-
"""mammo_masses_classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OORQFn0ZNdntWHJqQaB-bpFqEPLoOl87

## Predict whether a mammogram mass is benign or malignant

We'll be using the "mammographic masses" public dataset from the UCI repository (source: https://archive.ics.uci.edu/ml/datasets/Mammographic+Mass)

This data contains 961 instances of masses detected in mammograms, and contains the following attributes:


   1. BI-RADS assessment: 1 to 5 (ordinal)  
   2. Age: patient's age in years (integer)
   3. Shape: mass shape: round=1 oval=2 lobular=3 irregular=4 (nominal)
   4. Margin: mass margin: circumscribed=1 microlobulated=2 obscured=3 ill-defined=4 spiculated=5 (nominal)
   5. Density: mass density high=1 iso=2 low=3 fat-containing=4 (ordinal)
   6. Severity: benign=0 or malignant=1 (binominal)
   
BI-RADS is an assesment of how confident the severity classification is; it is not a "predictive" attribute and so we will discard it. The age, shape, margin, and density attributes are the features that we will build our model with, and "severity" is the classification we will attempt to predict based on those attributes.

Although "shape" and "margin" are nominal data types, which sklearn typically doesn't deal with well, they are close enough to ordinal that we shouldn't just discard them. The "shape" for example is ordered increasingly from round to irregular.

A lot of unnecessary anguish and surgery arises from false positives arising from mammogram results. If we can build a better way to interpret them through supervised machine learning, it could improve a lot of lives.

## Let's begin: prepare your data

Start by importing the mammographic_masses.data.txt file into a Pandas dataframe (hint: use read_csv) and take a look at it.
"""

import pandas as pd

masses_data = pd.read_csv('mammographic_masses.data.txt')
masses_data.head()

"""Make sure you use the optional parmaters in read_csv to convert missing data (indicated by a ?) into NaN, and to add the appropriate column names (BI_RADS, age, shape, margin, density, and severity):"""

masses_data = pd.read_csv('mammographic_masses.data.txt', na_values=['?'], 
                          names = ['BI-RADS', 'age', 'shape', 'margin', 'density', 'severity'])
masses_data.head()

"""Evaluate whether the data needs cleaning; your model is only as good as the data it's given. Hint: use describe() on the dataframe."""

masses_data.describe()

"""Let us handle missing values by simply dropping using dropna()."""

masses_data.dropna(inplace=True)
masses_data.describe()

"""Next you'll need to convert the Pandas dataframes into numpy arrays that can be used by scikit_learn. Create an array that extracts only the feature data we want to work with (age, shape, margin, and density) and another array that contains the classes (severity). You'll also need an array of the feature name labels."""

all_features = masses_data[['age', 'shape',
                             'margin', 'density']].values


all_classes = masses_data['severity'].values

feature_names = ['age', 'shape', 'margin', 'density']

all_features

"""Some of our models require the input data to be normalized, so go ahead and normalize the attribute data. Hint: use preprocessing.StandardScaler()."""

from sklearn import preprocessing

scaler = preprocessing.StandardScaler()
all_features_scaled = scaler.fit_transform(all_features)
all_features_scaled

"""## Decision Trees

start by creating a single train/test split of our data. Set aside 75% for training, and 25% for testing.
"""

import numpy
from sklearn.model_selection import train_test_split

numpy.random.seed(1234)

(training_inputs,
 testing_inputs,
 training_classes,
 testing_classes) = train_test_split(all_features_scaled, all_classes, test_size=0.25, random_state=1)

"""Now create a DecisionTreeClassifier and fit it to your training data."""

from sklearn.tree import DecisionTreeClassifier

clf= DecisionTreeClassifier(random_state=1, max_depth=3,criterion='entropy')

# Train the classifier on the training set
clf.fit(training_inputs, training_classes)

"""Display the resulting decision tree."""

from IPython.display import Image  
from sklearn.externals.six import StringIO  
from sklearn import tree
from pydotplus import graph_from_dot_data 

dot_data = StringIO()  
tree.export_graphviz(clf, out_file=dot_data,  
                         feature_names=feature_names)  
graph = graph_from_dot_data(dot_data.getvalue())  
Image(graph.create_png())

"""Measure the accuracy of the resulting decision tree model using your test data."""

clf.score(testing_inputs, testing_classes)

"""Now instead of a single train/test split, use K-Fold cross validation to get a better measure of your model's accuracy (K=10). Hint: use model_selection.cross_val_score"""

from sklearn.model_selection import cross_val_score

clf = DecisionTreeClassifier(random_state=1, max_depth=3, criterion="entropy")

cv_scores = cross_val_score(clf, all_features_scaled, all_classes, cv=10)
print(cv_scores)
cv_scores.mean()

